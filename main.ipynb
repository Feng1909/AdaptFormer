{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaptFormer\n",
    "\n",
    "论文地址：https://arxiv.org/abs/2205.13535\n",
    "\n",
    "## 简介：\n",
    "\n",
    "港大，腾讯AI实验室，港中文贡献的文章：AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. 研究人员认为最新的transformer文章做的是**same network with task-specific weight**工作，用的是同样的网络，但是对每个下游任务都要fine-tune模型，这样的模型是不可拓展的，每搞一个数据集就要在上边fully finetune, 尤其是现在像ViT-G/14这样有18亿参数的大模型，训练时的算力和存储负担很重。所以他们要搞**same network with almost same weights**, 不仅网络要一样，应用到下游任务，权重也尽可能一样。只需要训练很少的参数，其他大部分参数是固定的，这些固定的参数就可以跨任务共享。\n",
    "\n",
    "要做这件事需要构建一种高效的pileline去适配预训练模型到许多下游任务，他们的工作更像**VPT** (Visual Prompt Tuning)，VPT在patch embedding那里增加可学习的参数同时冻结整个主干只finetuen embedding部分，但本项目所作的工作能够大大超越VPT，如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f76529af9d20462d8f0ba44d48a15da578875da0808a47adb2c22863b4905f17)\n",
    "\n",
    "**AdaptFormer**方法在SSv2数据集上**全面打败**了**VPT**\n",
    "\n",
    "本文的方法和VPT**不同**的地方在于，**AdaptFormer**是加到Transformer的**MHSA**(multi-head self-attention layer)上：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c2c110235c8549cca3a79e92c774440a458afbca68f841b6902881da4c5fde38)\n",
    "\n",
    "\n",
    "下图为在各种数据集上本方法与VPT等方法的对比：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e967f3621daf4ca8853f72f5e4e584ef19bc846aa5e84118aabdce340ad4a934)\n",
    "\n",
    "\n",
    "最后文章希望可以激励更多研究者探索更加高效的fine-tuning方法到大型视觉模型上。\n",
    "\n",
    "## 数据集介绍：Cifar10\n",
    "\n",
    "链接：http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/15a8790a113d41418d6fc8563aeb4acd10da73b4b8c6488599fa9e7a01cc0833)\n",
    "\n",
    "**CIFAR-10**是一个更接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10 个类别的RGB彩色图片：**飞机**(airplane)、**汽车**(automobile)、**鸟类**(bird)、**猫**(cat)、**鹿**(deer)、**狗**(dog)、**蛙类**(frog)、马(horse)、**船**(ship)和**卡车**(truck).\n",
    "\n",
    "每个图片的尺寸为 $32\\times 32$，每个类别有**6000**个图像，数据集中一共有**50000**张训练图片和**10000**张测试图片。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.引入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import functional as F\n",
    "\n",
    "from paddle.utils.download import get_weights_path_from_url\n",
    "import pickle\n",
    "import numpy as np\n",
    "from paddle import callbacks\n",
    "from paddle.vision.transforms import (\n",
    "    ToTensor, RandomHorizontalFlip, RandomResizedCrop, SaturationTransform, Compose,\n",
    "    HueTransform, BrightnessTransform, ContrastTransform, RandomCrop, Normalize, RandomRotation, Resize\n",
    ")\n",
    "from paddle.vision.datasets import Cifar10\n",
    "from paddle.io import DataLoader\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay, MultiStepDecay, LinearWarmup\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, Dropout, BatchNorm, AdaptiveAvgPool2D, AvgPool2D\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.图像分块嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:07.755857Z",
     "iopub.status.busy": "2022-06-13T05:54:07.755345Z",
     "iopub.status.idle": "2022-06-13T05:54:07.763624Z",
     "shell.execute_reply": "2022-06-13T05:54:07.762756Z",
     "shell.execute_reply.started": "2022-06-13T05:54:07.755808Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 图像分块、Embedding\n",
    "class PatchEmbed(nn.Layer):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        # 原始大小为int，转为tuple，即：img_size原始输入224，变换后为[224,224]\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        # 图像块的个数\n",
    "        num_patches = (img_size[1] // patch_size[1]) * \\\n",
    "            (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        # kernel_size=块大小，即每个块输出一个值，类似每个块展平后使用相同的全连接层进行处理\n",
    "        # 输入维度为3，输出维度为块向量长度\n",
    "        # 与原文中：分块、展平、全连接降维保持一致\n",
    "        # 输出为[B, C, H, W]\n",
    "        self.proj = nn.Conv2D(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            \"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # [B, C, H, W] -> [B, C, H*W] ->[B, H*W, C]\n",
    "        x = self.proj(x).flatten(2).transpose((0, 2, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:13.127912Z",
     "iopub.status.busy": "2022-06-13T05:54:13.127414Z",
     "iopub.status.idle": "2022-06-13T05:54:13.137094Z",
     "shell.execute_reply": "2022-06-13T05:54:13.136472Z",
     "shell.execute_reply.started": "2022-06-13T05:54:13.127869Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 dim,\r\n",
    "                 num_heads=8,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 attn_drop=0.,\r\n",
    "                 proj_drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        self.num_heads = num_heads\r\n",
    "        head_dim = dim // num_heads\r\n",
    "        self.scale = qk_scale or head_dim**-0.5\r\n",
    "        # 计算 q,k,v 的转移矩阵\r\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\r\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\r\n",
    "        # 最终的线性层\r\n",
    "        self.proj = nn.Linear(dim, dim)\r\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        N, C = x.shape[1:]\r\n",
    "        # 线性变换\r\n",
    "        qkv = self.qkv(x).reshape((-1, N, 3, self.num_heads, C //\r\n",
    "                                   self.num_heads)).transpose((2, 0, 3, 1, 4))\r\n",
    "        # 分割 query key value\r\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\r\n",
    "        # Scaled Dot-Product Attention\r\n",
    "        # Matmul + Scale\r\n",
    "        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\r\n",
    "        # SoftMax\r\n",
    "        attn = nn.functional.softmax(attn, axis=-1)\r\n",
    "        attn = self.attn_drop(attn)\r\n",
    "        # Matmul\r\n",
    "        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\r\n",
    "        # 线性变换\r\n",
    "        x = self.proj(x)\r\n",
    "        x = self.proj_drop(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:16.495388Z",
     "iopub.status.busy": "2022-06-13T05:54:16.494894Z",
     "iopub.status.idle": "2022-06-13T05:54:16.502059Z",
     "shell.execute_reply": "2022-06-13T05:54:16.501258Z",
     "shell.execute_reply.started": "2022-06-13T05:54:16.495348Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入层：线性变换\n",
    "        x = self.fc1(x)\n",
    "        # 应用激活函数\n",
    "        x = self.act(x)\n",
    "        # Dropout\n",
    "        x = self.drop(x)\n",
    "        # 输出层：线性变换\n",
    "        x = self.fc2(x)\n",
    "        # Dropout\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.基础模块\n",
    "基于上面实现的 Attention、MLP 和下面的 DropPath 模块就可以组合出 Vision Transformer 模型的一个基础模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:18.958498Z",
     "iopub.status.busy": "2022-06-13T05:54:18.957762Z",
     "iopub.status.idle": "2022-06-13T05:54:18.964652Z",
     "shell.execute_reply": "2022-06-13T05:54:18.963938Z",
     "shell.execute_reply.started": "2022-06-13T05:54:18.958456Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob=0., training=False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\n",
    "    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n",
    "    random_tensor = paddle.floor(random_tensor)\n",
    "    output = x.divide(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Layer):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:21.040932Z",
     "iopub.status.busy": "2022-06-13T05:54:21.040435Z",
     "iopub.status.idle": "2022-06-13T05:54:21.050653Z",
     "shell.execute_reply": "2022-06-13T05:54:21.050050Z",
     "shell.execute_reply.started": "2022-06-13T05:54:21.040889Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer='nn.LayerNorm',\n",
    "                 epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n",
    "        # Multi-head Self-attention\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop)\n",
    "        # DropPath\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim,\n",
    "                       hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "        self.n_embd = 768\n",
    "        self.down_size = 64\n",
    "        self.down_proj = nn.Linear(self.n_embd, self.down_size)\n",
    "        self.non_linear_func = nn.ReLU()\n",
    "        self.up_proj = nn.Linear(self.down_size, self.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Self-attention， Add， LayerNorm\n",
    "        residual = x\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        # Feed Forward， Add， LayerNorm\n",
    "        # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        x = self.mlp(self.norm2(x))\n",
    "        \n",
    "        down = self.down_proj(x)\n",
    "        down = self.non_linear_func(down)\n",
    "        down = nn.functional.dropout(down, p=0.1)\n",
    "        up = self.up_proj(down)\n",
    "        # up = up * self.scale\n",
    "        output = up + residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.参数初始化配置、独立的不进行任何操作的网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:23.414767Z",
     "iopub.status.busy": "2022-06-13T05:54:23.414239Z",
     "iopub.status.idle": "2022-06-13T05:54:23.420149Z",
     "shell.execute_reply": "2022-06-13T05:54:23.419486Z",
     "shell.execute_reply.started": "2022-06-13T05:54:23.414727Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数初始化配置\n",
    "trunc_normal_ = nn.initializer.TruncatedNormal(std=.02)\n",
    "zeros_ = nn.initializer.Constant(value=0.)\n",
    "ones_ = nn.initializer.Constant(value=1.)\n",
    "\n",
    "# 将输入 x 由 int 类型转为 tuple 类型\n",
    "def to_2tuple(x):\n",
    "    return tuple([x] * 2)\n",
    "\n",
    "# 定义一个什么操作都不进行的网络层\n",
    "class Identity(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:26.318752Z",
     "iopub.status.busy": "2022-06-13T05:54:26.318104Z",
     "iopub.status.idle": "2022-06-13T05:54:26.332930Z",
     "shell.execute_reply": "2022-06-13T05:54:26.332355Z",
     "shell.execute_reply.started": "2022-06-13T05:54:26.318704Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 class_dim=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 norm_layer='nn.LayerNorm',\n",
    "                 epsilon=1e-5,\n",
    "                 **args):\n",
    "        super().__init__()\n",
    "        self.class_dim = class_dim\n",
    "\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        # 图片分块和降维，块大小为patch_size，最终块向量维度为768\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        # 分块数量\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # 可学习的位置编码\n",
    "        self.pos_embed = self.create_parameter(\n",
    "            shape=(1, num_patches + 1, embed_dim), default_initializer=zeros_)\n",
    "        self.add_parameter(\"pos_embed\", self.pos_embed)\n",
    "        # 人为追加class token，并使用该向量进行分类预测\n",
    "        self.cls_token = self.create_parameter(\n",
    "            shape=(1, 1, embed_dim), default_initializer=zeros_)\n",
    "        self.add_parameter(\"cls_token\", self.cls_token)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = np.linspace(0, drop_path_rate, depth)\n",
    "        # transformer\n",
    "        self.blocks = nn.LayerList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                epsilon=epsilon) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim,\n",
    "                              class_dim) if class_dim > 0 else Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed)\n",
    "        trunc_normal_(self.cls_token)\n",
    "        self.apply(self._init_weights)\n",
    "    # 参数初始化\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            zeros_(m.bias)\n",
    "            ones_(m.weight)\n",
    "    # 获取图像特征\n",
    "    def forward_features(self, x):\n",
    "        B = paddle.shape(x)[0]\n",
    "        # 将图片分块，并调整每个块向量的维度\n",
    "        x = self.patch_embed(x)\n",
    "        # 将class token与前面的分块进行拼接\n",
    "        cls_tokens = self.cls_token.expand((B, -1, -1))\n",
    "        x = paddle.concat((cls_tokens, x), axis=1)\n",
    "        # 将编码向量中加入位置编码\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        # 堆叠 transformer 结构\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        # LayerNorm\n",
    "        x = self.norm(x)\n",
    "        # 提取分类 tokens 的输出\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 获取图像特征\n",
    "        x = self.forward_features(x)\n",
    "        # 图像分类\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T05:54:31.022455Z",
     "iopub.status.busy": "2022-06-13T05:54:31.021892Z",
     "iopub.status.idle": "2022-06-13T05:54:35.607779Z",
     "shell.execute_reply": "2022-06-13T05:54:35.607183Z",
     "shell.execute_reply.started": "2022-06-13T05:54:31.022399Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 13:54:31.025111   232 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0613 13:54:31.030635   232 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1       [[1, 3, 32, 32]]      [1, 768, 2, 2]        590,592    \n",
      " PatchEmbed-1     [[1, 3, 32, 32]]       [1, 4, 768]             0       \n",
      "   Dropout-1       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-1      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-1        [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "   Dropout-2      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-2        [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "   Dropout-3       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-1      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-1       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-2      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-3        [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-1         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "   Dropout-4       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-4        [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-1         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-5        [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-1          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-6         [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-1        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-3      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-7        [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "   Dropout-5      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-8        [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "   Dropout-6       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-2      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-2       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-4      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-9        [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-2         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "   Dropout-7       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-10       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-2         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-11       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-2          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-12        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-2        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-5      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-13       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "   Dropout-8      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-14       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "   Dropout-9       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-3      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-3       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-6      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-15       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-3         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-10       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-16       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-3         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-17       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-3          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-18        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-3        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-7      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-19       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-11      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-20       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-12       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-4      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-4       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-8      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-21       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-4         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-13       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-22       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-4         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-23       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-4          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-24        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-4        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  LayerNorm-9      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-25       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-14      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-26       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-15       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-5      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-5       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-10      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-27       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-5         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-16       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-28       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-5         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-29       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-5          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-30        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-5        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-11      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-31       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-17      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-32       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-18       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-6      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-6       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-12      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-33       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-6         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-19       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-34       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-6         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-35       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-6          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-36        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-6        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-13      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-37       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-20      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-38       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-21       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-7      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-7       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-14      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-39       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-7         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-22       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-40       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-7         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-41       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-7          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-42        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-7        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-15      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-43       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-23      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-44       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-24       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-8      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-8       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-16      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-45       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-8         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-25       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-46       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-8         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-47       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-8          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-48        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-8        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-17      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-49       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-26      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-50       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-27       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Attention-9      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-9       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-18      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-51       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-9         [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-28       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-52       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "     Mlp-9         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-53       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-9          [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-54        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "    Block-9        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-19      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-55       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-29      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-56       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-30       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " Attention-10      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-10      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-20      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-57       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-10        [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-31       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-58       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "    Mlp-10         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-59       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-10         [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-60        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "   Block-10        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-21      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-61       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-32      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-62       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-33       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " Attention-11      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-11      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-22      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-63       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-11        [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-34       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-64       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "    Mlp-11         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-65       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-11         [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-66        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "   Block-11        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-23      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-67       [[1, 5, 768]]         [1, 5, 2304]        1,769,472   \n",
      "  Dropout-35      [[1, 12, 5, 5]]       [1, 12, 5, 5]            0       \n",
      "   Linear-68       [[1, 5, 768]]         [1, 5, 768]          590,592    \n",
      "  Dropout-36       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " Attention-12      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "  Identity-12      [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-24      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-69       [[1, 5, 768]]         [1, 5, 3072]        2,362,368   \n",
      "    GELU-12        [[1, 5, 3072]]        [1, 5, 3072]            0       \n",
      "  Dropout-37       [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-70       [[1, 5, 3072]]        [1, 5, 768]         2,360,064   \n",
      "    Mlp-12         [[1, 5, 768]]         [1, 5, 768]             0       \n",
      "   Linear-71       [[1, 5, 768]]          [1, 5, 64]          49,216     \n",
      "    ReLU-12         [[1, 5, 64]]          [1, 5, 64]             0       \n",
      "   Linear-72        [[1, 5, 64]]         [1, 5, 768]          49,920     \n",
      "   Block-12        [[1, 5, 768]]         [1, 5, 768]             0       \n",
      " LayerNorm-25      [[1, 5, 768]]         [1, 5, 768]           1,536     \n",
      "   Linear-73         [[1, 768]]           [1, 1000]           769,000    \n",
      "===========================================================================\n",
      "Total params: 87,577,576\n",
      "Trainable params: 87,577,576\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.93\n",
      "Params size (MB): 334.08\n",
      "Estimated Total Size (MB): 342.03\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 87577576, 'trainable_params': 87577576}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "vit = VisionTransformer()\n",
    "paddle.summary(vit, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-12T16:24:40.576598Z",
     "iopub.status.busy": "2022-06-12T16:24:40.575870Z",
     "iopub.status.idle": "2022-06-12T16:24:40.581309Z",
     "shell.execute_reply": "2022-06-12T16:24:40.580783Z",
     "shell.execute_reply.started": "2022-06-12T16:24:40.576564Z"
    },
    "tags": []
   },
   "source": [
    "### 9.自定义数据集处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-12T16:24:42.013564Z",
     "iopub.status.busy": "2022-06-12T16:24:42.012901Z",
     "iopub.status.idle": "2022-06-12T16:24:42.025978Z",
     "shell.execute_reply": "2022-06-12T16:24:42.025362Z",
     "shell.execute_reply.started": "2022-06-12T16:24:42.013531Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToArray(object):\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = np.transpose(img, [2, 0, 1])\n",
    "        img = img / 255.\n",
    "        return img.astype('float32')\n",
    "\n",
    "class RandomApply(object):\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.p < random.random():\n",
    "            return img\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "                                                                                                                    \n",
    "class LRSchedulerM(callbacks.LRScheduler):                                                                                                           \n",
    "    def __init__(self, by_step=False, by_epoch=True, warm_up=True):                                                                                                \n",
    "        super().__init__(by_step, by_epoch)                                                                                                                          \n",
    "        assert by_step ^ warm_up\n",
    "        self.warm_up = warm_up\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.by_epoch and not self.warm_up:\n",
    "            if self.model._optimizer and hasattr(\n",
    "                self.model._optimizer, '_learning_rate') and isinstance(\n",
    "                    self.model._optimizer._learning_rate, paddle.optimizer.lr.LRScheduler):                                                                                         \n",
    "                self.model._optimizer._learning_rate.step()                                                                                          \n",
    "                                                                                                                                                     \n",
    "    def on_train_batch_end(self, step, logs=None):                                                                                                   \n",
    "        if self.by_step or self.warm_up:                                                                                                                             \n",
    "            if self.model._optimizer and hasattr(\n",
    "                self.model._optimizer, '_learning_rate') and isinstance(\n",
    "                    self.model._optimizer._learning_rate, paddle.optimizer.lr.LRScheduler):                                                                                         \n",
    "                self.model._optimizer._learning_rate.step()\n",
    "            if self.model._optimizer._learning_rate.last_epoch >= self.model._optimizer._learning_rate.warmup_steps:\n",
    "                self.warm_up = False\n",
    "\n",
    "def _on_train_batch_end(self, step, logs=None):\n",
    "    logs = logs or {}\n",
    "    logs['lr'] = self.model._optimizer.get_lr()\n",
    "    self.train_step += 1\n",
    "    if self._is_write():\n",
    "        self._updates(logs, 'train')\n",
    "\n",
    "def _on_train_begin(self, logs=None):\n",
    "    self.epochs = self.params['epochs']\n",
    "    assert self.epochs\n",
    "    self.train_metrics = self.params['metrics'] + ['lr']\n",
    "    assert self.train_metrics\n",
    "    self._is_fit = True\n",
    "    self.train_step = 0\n",
    "\n",
    "callbacks.VisualDL.on_train_batch_end = _on_train_batch_end\n",
    "callbacks.VisualDL.on_train_begin = _on_train_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.在Cifar10数据集上训练模型\n",
    "使用Paddle自带的Cifar10数据集API加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = paddle.Model(VisionTransformer(class_dim=10))\n",
    "# 加载checkpoint\n",
    "# model.load('output/AdaptFormer/17.pdparams')\n",
    "MAX_EPOCH = 300\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 512\n",
    "CIFAR_MEAN = [0.5071, 0.4865, 0.4409]\n",
    "CIFAR_STD = [0.1942, 0.1918, 0.1958]\n",
    "DATA_FILE = None\n",
    "\n",
    "model.prepare(\n",
    "    paddle.optimizer.Momentum(\n",
    "        learning_rate=LinearWarmup(CosineAnnealingDecay(LR, MAX_EPOCH), 2000, 0., LR),\n",
    "        momentum=MOMENTUM,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=WEIGHT_DECAY),\n",
    "    paddle.nn.CrossEntropyLoss(),\n",
    "    paddle.metric.Accuracy(topk=(1,5)))\n",
    "\n",
    "# 定义数据集增强方式\n",
    "transforms = Compose([\n",
    "    RandomCrop(32, padding=4),\n",
    "    RandomApply(BrightnessTransform(0.1)),\n",
    "    RandomApply(ContrastTransform(0.1)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(15),\n",
    "    ToArray(),\n",
    "    Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    # Resize(size=384)\n",
    "])\n",
    "val_transforms = Compose([ToArray(), Normalize(CIFAR_MEAN, CIFAR_STD)])\n",
    "\n",
    "# 加载训练和测试数据集\n",
    "train_set = Cifar10(DATA_FILE, mode='train', transform=transforms)\n",
    "test_set = Cifar10(DATA_FILE, mode='test', transform=val_transforms)\n",
    "\n",
    "# 定义保存方式和训练可视化\n",
    "checkpoint_callback = paddle.callbacks.ModelCheckpoint(save_freq=1, save_dir='output/AdaptFormer')\n",
    "callbacks = [LRSchedulerM(),checkpoint_callback, callbacks.VisualDL('vis_logs/AdaptFormer.log')]\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    epochs=MAX_EPOCH, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    verbose=1, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "本次复现任务是一个很简单的小项目，是百度AI达人项目的一个课题，仅需要对MLP的运行逻辑部分进行修改。本次复现主要参考了github上作者的复现代码，并改写为PaddlePaddle框架代码，特别感谢百度李老师的指导！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
